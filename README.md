  <html lang="en">
    <head>

      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
      <title>Nick Bostrom's Home Page</title>
      <meta name="description" content="Oxford philosopher (videos, papers, interviews, bio, etc.)">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <link rel="stylesheet" href="css/bootstrap.min.css">
      <link rel="stylesheet" href="css/main.css">
      <link rel="stylesheet" href="css/custom.css">

       <!-- Google Analytics -->
       <script>
			  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

			  ga('create', 'UA-96504703-1', 'auto');
			  ga('send', 'pageview');
			</script>

       <!-- Google Analytics End-->
    </head>
    <body>

    <!--

      Major sections of this web page are marked with comments.

      Each comment begins with #!, so you can search that string to quickly
      move between sections.

    -->

    <!-- #! Website header -->
    <div class="container">
      <header class="site-header text-white">
        <div class="row">
          <div class="col-xxs-12 col-xs-4 col-md-4">
            <div class="site-header__portrait">
              <img id="nick-bostrom-portrait" src="nick-bostrom.jpg">
            </div>
          </div>

          <div class="col-xxs-12 col-xs-8 col-md-8 col-last col-site-header__title">

          <!-- <div class="col-xxs-12 col-xs-8 col-md-8 col-last col-site-header__title nl-header-title"> -->
            <!-- #! News Letter Subscription -->
            <!-- <form action="send.php" method="post">
            <div id="newsLetter" >
              <h4>Receive rare updates</h4>
              <div class="nl-container">
                <input type="email" name="nlEmail"  placeholder="&#9993; email address" required>
                <button>SEND ME NEWS LETTER</button>
              </div>
              <p>Once or twice a year if a new paper comes out</p>
            </div>
           </form> -->

            <div class="site-header__title">
              <h2>Nick Bostrom's Home Page</h2>
              <!-- <p class="lead">I think carefully about global priorities and big picture questions for humanity.</p>-->
              <p class="site-header__job-title">
								Professor, University of Oxford<br />
                Director, Future of Humanity Institute
              </p>
            </div>
            <div class="site-header__affiliations">
                <a href="https://www.fhi.ox.ac.uk" target="_blank" class="site-header__affiliation-logo"><img src="logo-fhi.png"></a>
                <a href="https://www.ox.ac.uk" target="_blank" class="site-header__affiliation-logo"><img src="logo-oxford.png"></a>
            </div>
          </div>
        </div>
      </header>
    </div>

    <!-- #! Newsletter
    <div class="container">
      <div class="updates bg-gray">
        <div class="row">
          Email box goes here
        </div>
      </div>
    </div> -->

    <!-- #! Updates -->
    <div class="container">
      <div class="updates bg-gray">
        <div class="row">
          <section class="col-md-7">

  <h3> January 2020 </h3>

            <p>Currently doing some work on the moral status of digital minds, and also thinking about various foundational issues in macrostrategy and anthropics. AI of course remains a strong focus. So much to do, so little time to do it.</p>
            <p>FHI is growing and is <a href="https://www.fhi.ox.ac.uk/vacancies/" target="_blank">recruiting</a> for top talent (both research and operations, all levels of seniority).</p>
            <p>For more, see e.g. <a href="http://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom" target="_blank">New Yorker profile</a>, <a href="#bio">Bio</a>, <a href="http://www.nickbostrom.com/cv.pdf" target="_self">CV</a>.</p>

          </section>

          <section class="col-md-5 col-last">
            <h3>Some recent additions</h3>
            <ul class="no-bullset">

              <li><a href="http://www.nickbostrom.com/papers/vulnerable.pdf" target="_self" onclick="ga('send','event','vwh','click','recent_additions')">The Vulnerable World Hypothesis</a>, in <em>Global Policy</em> </li>

              <li><a href="http://www.nickbostrom.com/papers/openness.pdf" target="_self">Strategic Implications of Openness in AI Development</a>, in <em>Global Policy</em></li>

              <li><a href="http://www.nickbostrom.com/papers/aipolicy.pdf" target="_self">Policy Desiderata in the Development of Machine Superintelligence</a>, working paper</li>

              <li><a href="http://www.nickbostrom.com/papers/porosity.pdf" target="_self">Hail Mary, Value Porosity, and Utility Diversification</a>, working paper</li>

              <li><a href="http://www.nickbostrom.com/papers/embryo.pdf" target="_self">Embryo Selection for Cognitive Enhancement: Curiosity or Game-changer?</a>, w/ Carl Shulman, in <em>Global Policy</em></li>

              <li><a href="http://www.nickbostrom.com/papers/unilateralist.pdf" target="_self">The Unilateralist's Curse: The Case for a Principle of Conformity</a>, w/ Anders Sandberg & Tom Douglas,  in <em>Social Epistemology</em></li>

            </ul>
         </section>
        </div>
      </div>
    </div>

    <!-- #! Selected papers -->
    <div class="container">
      <section class="selected-papers">
        <div class="row section-header">
          <div class="col-sm-12">
            <h2>Selected papers</h2>
          </div>
        </div>

        <div class="row">

          <!-- #! Selected papers: column 1 -->
          <div class="col-md-4">

            <!-- #! Selected papers: Ethics and Policy -->
            <h3>Ethics &amp; Policy</h3>

             <article>
              <h4><a href="http://www.nickbostrom.com/papers/aipolicy.pdf" target="_self">Policy Desiderata in the Development of Machine Superintelligence</a>&nbsp;<img src="new-icon-6.jpg" align="bottom"></h4>

              <p>What properties should we want a proposal for an AI governance pathway to have?</p>

							<footer>[w/ Allan Dafoe & Carrick Flynn] [version 4.3 (2018)] [forthcoming in Liao, S.M. (ed.): <strong>Ethics of Artificial Intelligence</strong> (Oxford University Press, 2019)]] [<a href="https://nickbostrom.com/papers/aipolicy.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <header>
                <a href="http://www.nickbostrom.com/fable/dragon.html" target="_self" class="header__image"><img src="dragon5.gif"></a>
                <h4><a href="http://www.nickbostrom.com/fable/dragon.html" target="_self">The Fable of the Dragon-Tyrant</a></h4>
              </header>

              <p>Recounts the Tale of a most vicious Dragon that ate thousands of people every day, and of the actions that the King, the People, and an assembly of Dragonologists took with respect thereto. </p>

							<footer>[<strong>J Med Ethics</strong>, Vol. 31, No. 5 (2005): 273-277] [translations: <a href="http://www.nickbostrom.com/fable/fable.pdf" target="_self">Czech</a>,
           <a href="http://www.nickbostrom.com/fable/draak-tiran.pdf" target="_self">Dutch</a>, Finnish, <a href="http://www.nickbostrom.com/fable/dragon-tyran.html" target="_self">French</a>, <a href="http://www.nickbostrom.com/fable/drachen-marchen.html" target="_self">German</a>, Hebrew, Italian, Russian, Slovenian, <a href="http://tendencias21.net/El-envejecimiento-es-una-tiranico-dragon-que-puede-ser-abatido_a703.html" target="_blank">Spanish</a>] [<a href="https://www.youtube.com/watch?v=cZYNADOHhVY" target="_blank">Animation</a>] [<a href="http://www.nickbostrom.com/fable/dragon.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/fable/dragon.pdf" target="_self">pdf</a>] [<a href="http://www.nickbostrom.com/fable/dragon.mp3" target="_self">mp3</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/ethics/statusquo.pdf" target="_self">The Reversal Test: Eliminating Status Quo Bias in Applied Ethics</a>&nbsp;<img src="star.jpg"></h4>

              <p>We present a heuristic for correcting for one kind of bias (status quo bias), which we suggest affects many of our judgments about the consequences of modifying human nature. We apply this heuristic to the case of cognitive enhancements, and argue that the consequentialist case for this is much stronger than commonly recognized.</p>

							<footer>[w/ Toby Ord] [<strong>Ethics</strong>, Vol. 116, No. 4 (2006): 656-680] [<a href="http://www.nickbostrom.com/ethics/statusquo.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/openness.pdf" target="_self">Strategic Implications of Openness in AI Development</a>&nbsp;<img src="new-icon-6.jpg" align="bottom"></h4>

              <p>An analysis of the global desirability of different forms of openness (including source code, science, data, safety techniques, capabilities, and goals). </p>

							<footer>[<strong>Global Policy</strong>, open access release 9 Feb 2017] [<a href="http://www.nickbostrom.com/papers/openness.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/astronomical/waste.html" target="_self">Astronomical Waste: The Opportunity Cost of Delayed Technological Development</a></h4>

              <p>Suns are illuminating and heating empty rooms, unused energy is being flushed down black holes, and our great common endowment of negentropy is being irreversibly degraded into entropy on a cosmic scale. These are resources that an advanced civilization could have used to create value-structures, such as sentient beings living worthwhile lives... </p>

							<footer>[<strong>Utilitas</strong>, Vol. 15, No. 3 (2003): 308-314] [translations: <a href="http://science.eduboard.com/astronomical-waste/" target="_blank">Russian</a>, <a href="https://altruismoeficaz.com.br/2019/11/26/desperdicio-astronomico-do-desenvolvimento-tecnologico/" target="_blank">Portuguese</a>] [<a href="http://www.nickbostrom.com/astronomical/waste.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/astronomical/waste.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <header>
                <a href="http://www.nickbostrom.com/ethics/infinite.pdf"  target="_self" class="header__image"><img src="aleph.jpg"></a>
                <h4><a href="http://www.nickbostrom.com/ethics/infinite.pdf" target="_self">Infinite Ethics</a></h4>
              </header>

              <p>Cosmology shows that we might well be living in an infinite universe that contains infinitely many happy and sad people. Given some assumptions, aggregative ethics implies that such a world contains an infinite amount of positive value and an infinite amount of negative value. But you can presumably do only a finite amount of good or bad. Since an infinite cardinal quantity is unchanged by the addition or subtraction of a finite quantity, it looks as though you can't change the value of the world. Aggregative consequentialism (and many other important ethical theories) are threatened by total paralysis. We explore a variety of potential cures, and discover that none works perfectly and all have serious side-effects. Is aggregative ethics doomed? </p>

							<footer>[<strong>Analysis and Metaphysics</strong>, Vol. 10 (2011): 9-59] [Original draft was available in 2003.] [<a href="http://www.nickbostrom.com/ethics/infinite.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/ethics/infinite.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/unilateralist.pdf" target="_self">The Unilateralist's Curse: The Case for a Principle of Conformity</a></h4>

              <p>In cases where several altruistic agents each have an opportunity to undertake some initiative, a phenomenon arises that is analogous to the winner's curse in auction theory. To combat this problem, we propose a principle of conformity. It has applications in technology policy and many other areas. </p>

							<footer>[Nick Bostrom, Thomas Douglas & Anders Sandberg (2016) The Unilateralist’s Curse and the Case for a Principle of Conformity, <strong>Social Epistemology</strong>, 30:4, 350-371, DOI: 10.1080/02691728.2015.1108373] [<a href="https://nickbostrom.com/papers/unilateralist.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/ethics/dignity-enhancement.pdf" target="_self">Dignity and Enhancement</a></h4>

              <p>Does human enhancement threaten our dignity as some have asserted? Or could our dignity perhaps be technologically enhanced? After disentangling several different concepts of dignity, this essay focuses on the idea of dignity as a quality (a kind of excellence admitting of degrees). The interactions between enhancement and dignity as a quality are complex and link into fundamental issues in ethics and value theory. </p>

							<footer>[<strong>Human Dignity and Bioethics: Essays Commissioned by the President&rsquo;s Council on Bioethics</strong> (Washington, D.C., 2008): 173-207] [<a href="http://www.nickbostrom.com/ethics/dignity-enhancement.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/ethics/dignity.html" target="_self">In Defense of Posthuman Dignity</a></h4>

              <p>Brief paper, critiques a host of bioconservative pundits who believe that enhancing human capacities and extending human healthspan would undermine our dignity. </p>

							<footer>[<strong>Bioethics</strong>, Vol. 19, No. 3 (2005): 202-214] [translations: Italian, Slovenian, <a href="http://www.nickbostrom.com/translations/Dignidade.pdf" target="_self">Portuguese</a>] [Was chosen for inclusion in a special anthology of the best papers published in this journal in the past two decades] [<a href="http://www.nickbostrom.com/ethics/dignity.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/ethics/dignity.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <header>
                <a href="https://www.amazon.com/dp/0199594961" target="_blank" class="book__cover"><img src="images/hccover2.jpg"></a>
                <h4><a href="https://www.amazon.com/dp/0199594961" target="_blank">Human Enhancement</a></h4>
              </header>
              <p>Original essays by various prominent moral philosophers on the ethics of human enhancement. </p>

							<footer>[Eds. Nick Bostrom & Julian Savulescu (<strong>Oxford University Press</strong>, 2009)]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/ethics/human-enhancement-ethics.pdf" target="_self">Enhancement Ethics: The State of the Debate</a></h4>

              <p>The introductory chapter from the book: 1-22 </p>

							<footer>[w/ Julian Savulescu] [<a href="http://www.nickbostrom.com/ethics/human-enhancement-ethics.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/ethics/genetic.html" target="_self">Human Genetic Enhancements: A Transhumanist Perspective</a></h4>

              <p>A transhumanist ethical framework for public policy regarding genetic enhancements, particularly human germ-line genetic engineering </p>

							<footer>[<strong>Journal of Value Inquiry</strong>, Vol. 37, No. 4 (2003): 493-506] [<a href="http://www.nickbostrom.com/ethics/genetic.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/ethics/genetic.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/ethics/human-enhancement.pdf" target="_self">Ethical Issues in Human Enhancement</a></h4>

              <p>Anthology chapter on the ethics of human enhancement </p>

							<footer>[w/ Rebecca Roache] [<strong>New Waves in Applied Ethics</strong>, ed. Jesper Ryberg et al. (Palgrave Macmillan, 2008): 120-152] [<a href="http://www.nickbostrom.com/ethics/human-enhancement.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/ethics/human-enhancement.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/ethics/artificial-intelligence.pdf" target="_self">The Ethics of Artificial Intelligence</a></h4>

              <p>Overview of ethical issues raised by the possibility of creating intelligent machines. Questions relate both to ensuring such machines do not harm humans and to the moral status of the machines themselves. </p>

							<footer>[w/ Eliezer Yudkowsky] [<strong>Cambridge Handbook of Artificial Intelligence</strong>, eds. William Ramsey &amp; Keith Frankish (Cambridge University Press)] [translation: <a href="http://www.ierfh.org/br.txt/EticaDaIA2011.pdf" target="_blank">Portuguese</a>] [<a href="http://www.nickbostrom.com/ethics/artificial-intelligence.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/ethics/ai.html" target="_self">Ethical Issues In Advanced Artificial Intelligence</a></h4>

              <p>Some cursory notes; not very in-depth. </p>

							<footer>[<strong>Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence</strong>, Vol. 2, ed. I. Smit et al., Int. Institute of Advanced Studies in Systems Research and Cybernetics, 2003, 12-17] [translation: Italian] [<a href="http://www.nickbostrom.com/ethics/ai.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/ethics/ai.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/smart-policy.pdf" target="_self">Smart Policy: Cognitive Enhancement and the Public Interest</a></h4>

              <p>Short article summarizing some of the key issues and offering specific recommendations, illustrating the opportunity and need for "smart policy": the integration into public policy of a broad-spectrum of approaches aimed at protecting and enhancing cognitive capacities and epistemic performance of individuals and institutions. </p>

							<footer>[w/ Rebecca Roache] [<strong>Enhancing Human Capacities</strong>, eds. J. Savulescu, R. ter Muelen, &amp; G. Kahane (Wiley-Blackwell, 2011)] [<a href="http://www.nickbostrom.com/papers/smart-policy.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/ethics/life-extension.html" target="_self">Recent Developments in the Ethics, Science, and Politics of Life-Extension</a></h4>

              <p>A review/commentary on The Fountain of Youth (OUP, 2004). </p>

							<footer>[<strong>Aging Horizons</strong>, No. 3 (2005): 28-34] [<a href="http://www.nickbostrom.com/ethics/life-extension.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/ethics/life-extension.pdf" target="_self">pdf</a>]</footer>
            </article>

            <!-- #! Selected papers: Transhumanism -->
            <h3>Transhumanism</h3>

            <article>
              <h4><a href="http://www.nickbostrom.com/posthuman.pdf" target="_self">Why I Want to be a Posthuman When I Grow Up</a></h4>

              <p>After some definitions and conceptual clarification, I argue for two theses. First, some posthuman modes of being would be extremely worthwhile. Second, it could be good for human beings to become posthuman. </p>

							<footer>[<strong>Medical Enhancement and Posthumanity</strong>, eds. Bert Gordijn and Ruth Chadwick (Springer, 2008): 107-137] [<a href="http://www.nickbostrom.com/posthuman.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <header>
                <a href="http://www.nickbostrom.com/utopia.html"  target="_self" class="header__image"><img src="utopia1.png"></a>
                <h4><a href="http://www.nickbostrom.com/utopia.html" target="_self">Letter from Utopia</a>&nbsp;<img src="new-icon-6.jpg" align="bottom"></h4>
              </header>

              <p>The good life: just how good could it be? A vision of the future from the future. </p>

              <footer>This is an improved version (v. 3.0) from 2019
                <br />[translations: <a href="http://www.nickbostrom.com/translations/utopie.pdf" target="_self">French</a>, Italian, <a href="https://nickbostrom.com/translations/Level_a_utopia.pdf" target="_self">Hungarian</a>, <a href="https://nickbostrom.com/translations/Russian_utopia.pdf" target="_self">Russian</a>] [<a href="http://www.nickbostrom.com/utopia.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/utopia.pdf" target="_self">pdf</a>] [<a href="http://www.nickbostrom.com/utopia.mp3" target="_self">mp3</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/views/transhumanist.pdf" target="_self">The Transhumanist FAQ</a></h4>

              <p>The revised version 2.1. The document represents an effort to develop a broadly based consensus articulation of the basics of responsible transhumanism. Some one hundred people collaborated with me in creating this text. Feels like from another era.</p>

							<footer>[translations: German, Hungarian, Dutch, Russian, Polish, <a href="http://transhumanismi.org/ukk" target="_blank">Finnish</a>, Greek, Italian] [<a href="http://www.nickbostrom.com/views/transhumanist.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/ethics/values.html" target="_self">Transhumanist Values</a></h4>

              <p>Wonderful ways of being may be located in the "posthuman realm", but we can't reach them. If we enhance ourselves using technology, however, we can go out there and realize these values. This paper sketches a transhumanist axiology. </p>

							<footer>[<strong>Ethical Issues for the 21st Century</strong>, ed. Frederick Adams, Philosophy Documentation Center Press, 2003; reprinted in <strong>Review of Contemporary Philosophy</strong>, Vol. 4, May (2005)]
                [translations: <a href="http://www.racjonalista.pl/kk.php/s,6014" target="_blank">Polish</a>, <a href="http://www.ierfh.org/br.txt/ValoresTranshumanistas2005.pdf" target="_blank">Portuguese</a>, <a href="translations/ethics/valores-transhumanistas-iet.pdf" target="_self">Spanish</a>] [<a href="http://www.nickbostrom.com/ethics/values.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/ethics/values.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/history.pdf" target="_self">A History of Transhumanist Thought</a></h4>

              <p>The human desire to acquire new capacities, to extend life and overcome obstacles to happiness is as ancient as the species itself. But transhumanism has emerged gradually as a distinctive outlook, with no one person being responsible for its present shape. Here's one account of how it happened. </p>

							<footer>[<strong>Journal of Evolution and Technology</strong>, Vol. 14, No. 1 (2005)] [translation: Spanish] [<a href="http://www.nickbostrom.com/papers/history.pdf" target="_self">pdf</a>]</footer>
            </article>
          </div>

          <!-- #! Selected papers: column 2 -->
          <div class="col-md-4">

            <!-- #! Selected papers: Risk &amp; the Future -->
            <h3>Risk &amp; The Future</h3>

            <article>
             <h4><a href="http://www.nickbostrom.com/papers/vulnerable.pdf" target="_self" onclick="ga('send','event','vwh','click','the_future')">The Vulnerable World Hypothesis</a>&nbsp;<img src="star.jpg">&nbsp;<img src="new-icon-6.jpg" align="bottom"></h4>

             <p>Is there a level of technology at which civilization gets destroyed by default?</p>

             <footer>
               [<strong>Global Policy</strong>, Vol. 10, Iss. 4 (Nov 2019): 455-476]
               [translation: <a href="https://nickbostrom.com/translations/vulnerable-chinese.pdf" target="_self" onclick="ga('send','event','vwh','click','the_future_chinese')">Chinese</a>]
							 [<a href="https://doi.org/10.1111/1758-5899.12718" target="_blank" rel="noopener" onclick="ga('send','event','vwh','click','the_future_doi')">html</a>]
							 [<a href="https://nickbostrom.com/papers/vulnerable.pdf" target="_self" onclick="ga('send','event','vwh','click','the_future_footer')">pdf</a>]
							 [<a href="https://www.ted.com/talks/nick_bostrom_how_civilization_could_destroy_itself_and_4_ways_we_could_prevent_it" target="_blank" onclick="ga('send','event','vwh','click','ted2019_paper')">TED video</a>]
             </footer>
           </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/extraterrestrial.pdf" target="_self">Where Are They? Why I hope the search for extraterrestrial life finds nothing</a>&nbsp;<img src="star.jpg"></h4>

              <p>Discusses the Fermi paradox, and explains why I hope we find no signs of life, whether extinct or still thriving, on Mars or anywhere else we look. </p>

							<footer>[<strong>Technology Review</strong>, May/June issue (2008): 72-77] [translation: Italian] [<a href="http://www.nickbostrom.com/extraterrestrial.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.existential-risk.org/concept.html" target="_blank">Existential Risk Reduction as Global Priority</a></h4>

              <p>Existential risks are those that threaten the entire future of humanity. This paper elaborates the concept of existential risk and its relation to basic issues in axiology and develops an improved classification scheme for such risks. It also describes some of the theoretical and practical challenges posed by various existential risks and suggests a new way of thinking about the ideal of sustainability. </p>

							<footer>[<strong>Global Policy</strong>, Vol. 4, No. 3 (2013): 15-31] [translation: <a href="http://www.ierfh.org/br.txt/PrevencaoContraORiscoExistencial2012.pdf" target="_blank">Portuguese</a>] [<a href="http://www.existential-risk.org/concept.html" target="_blank">html</a>] [<a href="http://www.existential-risk.org/concept.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://arxiv.org/abs/astro-ph/0512204" target="_blank">How Unlikely is a Doomsday Catastrophe?</a></h4>

              <p>Examines the risk from physics experiments and natural events to the local fabric of spacetime. Argues that the Brookhaven report overlooks an observation selection effect. Shows how this limitation can be overcome by using data on planet formation rates. </p>

							<footer>[w/ Max Tegmark] [expanded; <strong>Nature</strong>, Vol. 438 (2005): 754] [translation: <a href="http://www.proza.ru/texts/2007/04/11-348.html" target="_blank">Russian</a>] [<a href="https://arxiv.org/pdf/astro-ph/0512204.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/future.html" target="_self">The Future of Humanity</a></h4>

              <p>This paper discusses four families of scenarios for humanity&rsquo;s future: extinction, recurrent collapse, plateau, and posthumanity. </p>

							<footer>[<strong>New Waves in Philosophy of Technology</strong>, eds. Jan-Kyrre Berg Olsen, Evan Selinger &amp; Soren Riis (Palgrave McMillan, 2009) [<a href="http://www.nickbostrom.com/papers/future.pdf" target="_self">pdf</a>] [<a href="http://www.nickbostrom.com/papers/future.html" target="_self">html</a>]</footer>
            </article>

            <article>
              <header>
                <a href="http://www.global-catastrophic-risks.com/book.html" target="_blank"  class="book__cover"><img src="images/gcrcover.jpg"></a>
                <h4><a href="http://www.global-catastrophic-risks.com/book.html">Global Catastrophic Risks</a></h4>
              </header>
              <p>Twenty-six leading experts look at the gravest risks facing humanity in the 21st century, including natural catastrophes, nuclear war, terrorism, global warming, biological weapons, totalitarianism, advanced nanotechnology, general artificial intelligence, and social collapse. The book also addresses overarching issues&mdash;policy responses and methods for predicting and managing catastrophes. Foreword by Lord Martin Rees. </p>

							<footer>[Eds. Nick Bostrom &amp; Milan Cirkovic (<strong>Oxford University Press</strong>, 2008)]. Introduction chapter free here [<a href="http://www.global-catastrophic-risks.com/docs/Chap01.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/fut/evolution.html" target="_self">The Future of Human Evolution</a></h4>

              <p>This paper explores some dystopian scenarios where freewheeling evolutionary developments, while continuing to produce complex and intelligent forms of organization, lead to the gradual elimination of all forms of being worth caring about. We then discuss how such outcomes could be avoided and argue that under certain conditions the only possible remedy would be a globally coordinated effort to control human evolution by adopting social policies that modify the default fitness function of future life forms. </p>

							<footer>[<strong>Death and Anti-Death</strong>, ed. Charles Tandy (Ria University Press, 2005): 339-371] [<a href="http://www.nickbostrom.com/fut/evolution.pdf" target="_self">pdf</a>] [<a href="http://www.nickbostrom.com/fut/evolution.html" target="_self">html</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/revolutions.pdf" target="_self">Technological Revolutions: Ethics and Policy in the Dark</a></h4>

              <p>Technological revolutions are among the most important things that happen to humanity. This paper discusses some of the ethical and policy issues raised by anticipated technological revolutions, such as nanotechnology. </p>

							<footer>[<strong>Nanoscale: Issues and Perspectives for the Nano Century</strong>, eds. Nigel M. de S. Cameron &amp; M. Ellen Mitchell (John Wiley, 2007): 129-152] [<a href="http://www.nickbostrom.com/revolutions.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/existential/risks.html" target="_self">Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards</a></h4>

              <p>Existential risks are ways in which we could screw up badly and permanently. Remarkably, relatively little serious work has been done in this important area. The point, of course, is not to welter in doom and gloom but to better understand where the biggest dangers are so that we can develop strategies for reducing them. </p>

							<footer>[<strong>Journal of Evolution and Technology</strong>, Vol. 9, No. 1 (2002)] [<a href="http://www.nickbostrom.com/existential/risks.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/existential/risks.pdf" target="_self">pdf</a>] [translations: <a href="http://www.proza.ru/texts/2007/04/04-210.html" target="_blank">Russian</a>, Belarusian]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/information-hazards.pdf" target="_self">Information Hazards: A Typology of Potential Harms from Knowledge</a></h4>

              <p>Information hazards are risks that arise from the dissemination or the potential dissemination of true information that may cause harm or enable some agent to cause harm. Such hazards are often subtler than direct physical threats, and, as a consequence, are easily overlooked. They can, however, be important. </p>

							<footer>[<strong>Review of Contemporary Philosophy</strong>, Vol. 10 (2011): 44-79 (first version: 2009)] [<a href="http://www.nickbostrom.com/information-hazards.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/fut/singleton.html" target="_self">What is a Singleton?</a></h4>

              <p>Concept describing a kind of social structure. </p>

							<footer>[<strong>Linguistic and Philosophical Investigations</strong>, Vol. 5, No. 2 (2006): 48-54] [translation: <a href="http://www.nickbostrom.com/translations/Czym%20jest%20singleton.pdf" target="_self">Polish</a>]</footer>
            </article>

            <!-- #! Selected papers: Technology Issues -->
            <h3>Technology Issues</h3>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/embryo.pdf" target="_self">Embryo Selection for Cognitive Enhancement: Curiosity or Game-changer?</a></h4>

              <p>The embryo selection during IVF can be vastly potentiated when the technology for stem-cell derived gametes becomes available for use in humans. This would enable iterated embryo selection (IES), compressing the effective generation time in a selection program from decades to months. </p>

							<footer>[w/ Carl Shulman] [<strong>Global Policy</strong>, Vol. 5, No. 1 (2014): 85-92] [<a href="http://www.nickbostrom.com/papers/embryo.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/aievolution.pdf" target="_self">How Hard is AI? Evolutionary Arguments and Selection Effects</a></h4>

              <p>Some have argued that because blind evolutionary processes produced human intelligence on Earth, it should be feasible for clever human engineers to create human-level artificial intelligence in the not-too-distant future. We evaluate this argument. </p>

							<footer>[w/ Carl Shulman] [<strong>J. Consciousness Studies</strong>, Vol. 19, No. 7-8 (2012): 103-130] [<a href="http://www.nickbostrom.com/aievolution.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/evolution.pdf" target="_self">The Wisdom of Nature: An Evolutionary Heuristic for Human Enhancement</a></h4>

              <p>Human beings are a marvel of evolved complexity. Such systems can be difficult to enhance. Here we describe a heuristic for identifying and evaluating the practicality, safety and efficacy of potential human enhancements, based on evolutionary considerations. </p>

							<footer>[w/ Anders Sandberg] [<strong>Enhancing Humans</strong>, eds. Julian Savulescu &amp; Nick Bostrom (Oxford University Press, 2009): 365-416 [<a href="http://www.nickbostrom.com/evolution.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/superintelligentwill.pdf" target="_self">The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents</a></h4>

              <p>Presents two theses, the orthogonality thesis and the instrumental convergence thesis, that help understand the possible range of behavior of superintelligent agents - also pointing to some potential dangers in building such an agent. </p>

							<footer>[<strong>Minds and Machines</strong>, Vol. 22 (2012): 71-84] [<a href="http://www.nickbostrom.com/superintelligentwill.pdf" target="_self">pdf</a>] [translation: <a href="http://www.ierfh.org/br.txt/VontadeSuperinteligente2012.pdf" target="_blank">Portuguese</a>]</footer>
            </article>

            <article>
              <header>
                <a href="http://www.fhi.ox.ac.uk/Reports/2008-3.pdf" target="_blank" class="book__cover"><img src="images/wbecover.png"></a>
                <h4><a href="http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf" target="_blank">Whole Brain Emulation: A Roadmap</a></h4>
              </header>
              <p>A 130-page report on the technological prerequisites for whole brain emulation (aka "mind uploading"). </p>

							<footer>[w/ Anders Sandberg] [<strong>Technical Report</strong> #2008-3, Future of Humanity Institute, Oxford University (2008)] [<a href="http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/converging.pdf" target="_self">Converging Cognitive Enhancements</a></h4>

              <p>Cognitive enhancements in the context of converging technologies. </p>

							<footer>[w/ Anders Sandberg] [<strong>Annals of the New York Academy of Sciences</strong>, Vol. 1093 (2006): 201-207] [<a href="http://www.nickbostrom.com/papers/converging.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/porosity.pdf" target="_self">Hail Mary, Value Porosity, and Utility Diversification</a> </h4>

              <p>Some new ideas related to the challenge of endowing a hypothetical future superintelligent AI with values that would cause it to act in ways that are beneficial. Paper is somewhat obscure. </p>

							<footer>[December, 2014] [<a href="http://www.nickbostrom.com/papers/porosity.pdf" target="_self">pdf</a>]</footer>
             </article>

            <article>
              <h4><a href="http://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf" target="_blank">Racing to the Precipice: a Model of Artificial Intelligence Development</a></h4>

              <p>Game theory model of a technology race to develop AI. Participants skimp on safety precautions to get there first. Analyzes factors that determine level of risk in the Nash equilibrium. </p>

							<footer>[w/ Stuart Armstrong & Carl Shulman] [<strong>Technical Report</strong> #2013-1, Future of Humanity Institute, Oxford University: 1-8] [<strong>AI and Society</strong>, Vol. 31, No.2 (2016): 201-206] [<a href="http://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/oracle.pdf" target="_self">Thinking Inside the Box: Controlling and Using Oracle AI</a></h4>

              <p>Preliminary survey of various issues related to the idea of using boxing methods to safely contain a superintelligent oracle AI. </p>

							<footer>[w/ Stuart Armstrong &amp; Anders Sandberg] [<strong>Minds and Machines</strong>, Vol. 22, No. 4 (2012): 299-324] [<a href="http://www.nickbostrom.com/papers/oracle.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/survey.pdf" target="_self">Future Progress in Artificial Intelligence: A Survey of Expert Opinion</a></h4>

              <p>Some polling data. </p>

							<footer>[<strong>Fundamental Issues of Artificial Intelligence</strong>, ed. V. M&uuml;ller (Springer, 2016)] [w/ Vincent M&uuml;ller] [<a href="http://www.nickbostrom.com/papers/survey.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/cognitive.pdf" target="_self">Cognitive Enhancement: Methods, Ethics, Regulatory Challenges</a></h4>

              <p>Cognitive enhancement comes in many diverse forms. In this paper, we survey the current state of the art in cognitive enhancement methods and consider their prospects for the near-term future. We then review some of ethical issues arising from these technologies. We conclude with a discussion of the challenges for public policy and regulation created by present and anticipated methods for cognitive enhancement. </p>

							<footer>[w/ Anders Sandberg] [<strong>Science and Engineering Ethics</strong>, Vol. 15 (2009): 311-341] [<a href="http://www.nickbostrom.com/cognitive.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <header>
                <a href="https://www.simulation-argument.com" target="_blank" class="header__image"><img src="sim2.jpg"></a>
                <h4><a href="https://www.simulation-argument.com" target="_blank">Are You Living in a Computer Simulation?</a>&nbsp;<img src="star.jpg"></h4>
              </header>

              <p>This paper argues that at least one of the following propositions is true: (1) the human species is very likely to go extinct before reaching the posthuman stage; (2) any posthuman civilization is extremely unlikely to run significant number of simulations or (variations) of their evolutionary history; (3) we are almost certainly living in a computer simulation. It follows that the na&iuml;ve transhumanist dogma that there is a significant chance that we will one day become posthumans who run ancestor-simulations is false, unless we are currently living in a simulation. A number of other consequences of this result are also discussed. </p>

							<footer>[<strong>Philosophical Quarterly</strong>, Vol. 53, No. 211 (2003): 243-255] [<a href="https://www.simulation-argument.com/simulation.pdf" target="_blank">pdf</a>] [<a href="https://www.simulation-argument.com/simulation.html" target="_blank">html</a>] [Also with a <a href="https://www.simulation-argument.com/weathersonreply.pdf" target="_blank">Reply to Brian Weatherson</a>'s comments [<strong>Philosophical Quarterly</strong>, Vol. 55, No. 218 (2009): 90-97]; and a <a href="https://www.simulation-argument.com/brueckner.pdf" target="_blank">Reply to Anthony Brueckner</a>, <strong>[Analysis</strong>, Vol. 69, No. 3 (2009): 458-461]. And a <a href="https://www.simulation-argument.com/patch.pdf" target="_blank">new paper</a> w/ Marcin Kulczycki [<strong>Analysis</strong>, Vol. 71, No.1 (2011): 54-61].</footer>
            </article>

          </div>

          <!-- #! Selected papers: column 3 -->
          <div class="col-md-4 col-last">
            <h3>The New Book</h3>

            <article>
              <header>
                <a href="https://www.amazon.com/gp/product/0198739834" target="_blank" class="book__cover"><img src="superintelligence-cover-preorder4.jpg"></a>
                <h4><a href="https://www.amazon.com/gp/product/0198739834" target="_blank">Superintelligence: Paths, Dangers, Strategies</a>&nbsp;<img src="star.jpg"></h4>
              </header>
              <p><img src="nyt.png" class="pull-right" style="width: 100px;"> Superintelligence is out in paperback. Buy many copies now! </p>

							<footer>[<strong>Oxford University Press</strong>, 2014]</footer>
            </article>

            <blockquote>
              &ldquo;I highly recommend this book.&rdquo;<cite>&mdash;Bill Gates</cite>
            </blockquote>

            <blockquote>
              &ldquo;terribly important &hellip; groundbreaking&rdquo; &ldquo;extraordinary sagacity and clarity, enabling him to combine his wide-ranging knowledge over an impressively broad spectrum of disciplines &ndash; engineering, natural sciences, medicine, social sciences and philosophy &ndash; into a comprehensible whole&rdquo; &ldquo;If this book gets the reception that it deserves, it may turn out the most important alarm bell since Rachel Carson's Silent Spring from 1962, or ever.&rdquo;<cite>&mdash;Olle Haggstrom, Professor of Mathematical Statistics</cite>
            </blockquote>

            <blockquote>
              &ldquo;Nick Bostrom makes a persuasive case that the future impact of AI is perhaps the most important issue the human race has ever faced. &hellip; It marks the beginning of a new era.&rdquo;<cite>&mdash;Stuart Russell, Professor of Computer Science, University of California, Berkley</cite>
            </blockquote>

            <blockquote>
              &ldquo;Those disposed to dismiss an 'AI takeover' as science fiction may think again after reading this original and well-argued book.&rdquo; <cite>&mdash;Martin Rees, Past President, Royal Society</cite>
            </blockquote>

            <blockquote>
              &ldquo;Worth reading&hellip;. We need to be super careful with AI. Potentially more dangerous than nukes&rdquo;<cite>&mdash;Elon Musk</cite>
            </blockquote>

            <blockquote>
              &ldquo;There is no doubting the force of [Bostrom's] arguments &hellip; the problem is a research challenge worthy of the next generation's best mathematical talent. Human civilisation is at stake.&rdquo; <cite>&mdash;Financial Times</cite>
            </blockquote>

            <blockquote>
              &ldquo;This superb analysis by one of the world's clearest thinkers tackles one of humanity's greatest challenges: if future superhuman artificial intelligence becomes the biggest event in human history, then how can we ensure that it doesn't become the last?&rdquo; <cite>&mdash;Professor Max Tegmark, MIT</cite>
            </blockquote>

            <blockquote>
              &ldquo;a damn hard read&rdquo; <cite>&mdash;The Telegraph</cite>
            </blockquote>

            <!-- #! Selected papers: Anthropics and Probability  -->
            <h3>Anthropics &amp; Probability</h3>

            <article>
              <header>
                <a href="http://www.anthropic-principle.com/book/book.html" target="_blank" class="book__cover"><img src="images/abcover.jpg"></a>
                <h4><a href="http://www.anthropic-principle.com/book/book.html" target="_blank">Anthropic Bias: Observation Selection Effects in Science and Philosophy</a>&nbsp;<img src="star.jpg"></h4>
              </header>
              <p>Failure to consider observation selection effects result in a kind of bias that infest many branches of science and philosophy. This book presented the first mathematical theory for how to correct for these biases. It also discusses some implications for cosmology, evolutionary biology, game theory, the foundations of quantum mechanics, the Doomsday argument, the Sleeping Beauty problem, the search for extraterrestrial life, the question of whether God exists, and traffic planning. </p>

							<footer>[Complete book now available for <a href="http://www.anthropic-principle.com/book/book.html" target="_blank">free online</a>; also out as <a href="https://www.amazon.com/gp/product/0415883946" target="_blank">paperback</a>; there is also a brief <a href="http://www.anthropic-principle.com/primer.html" target="_blank">primer</a>. [primer translation: Belarusian] [<strong>Routledge</strong>, 2002]</footer>
            </article>

            <article>
              <h4><a href="http://www.anthropic-principle.com/preprints/cos/big.html" target="_blank">Self-Locating Belief in Big Worlds: Cosmology's Missing Link to Observation</a></h4>

              <p>Current cosmological theories say that the world is so big that all possible observations are in fact made. But then, how can such theories be tested? What could count as negative evidence? To answer that, we need to consider observation selection effects. </p>

							<footer>[<strong>Journal of Philosophy</strong>, Vol. 99, No. 12 (2002): 607-623] [<a href="http://www.anthropic-principle.com/preprints/cos/big.html" target="_blank">html</a>] [<a href="http://www.anthropic-principle.com/preprints/cos/big2.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://anthropic-principle.com/preprints/mys/mysteries.pdf" target="_blank">The Mysteries of Self-Locating Belief and Anthropic Reasoning</a></h4>

              <p>Summary of some of the difficulties that a theory of observation selection effects faces and sketch of a solution. </p>

							<footer>[<strong>Harvard Review of Philosophy</strong>, Vol. 11, Spring (2003): 59-74] [<a href="http://anthropic-principle.com/preprints/mys/mysteries.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/anthropicshadow.pdf" target="_self">Anthropic Shadow: Observation Selection Effects and Human Extinction Risks</a></h4>

              <p>"Anthropic shadow" is an observation selection effect that prevent observers from observing certain kinds of catastrophes in their recent geological and evolutionary past. We risk underestimating the risk of catastrophe types that lie in this shadow.</p>

							<footer>[w/ Milan Cirkovic & Anders Sandberg] [<strong>Risk Analysis</strong>, Vol. 30, No. 10 (2010): 1495-1506] [Won best paper of the year award by the journal editors] [translation: <a href="http://www.scribd.com/doc/48444529/anthropicshadow2" target="_blank">Russian</a>] [<a href="http://www.nickbostrom.com/papers/anthropicshadow.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://anthropic-principle.com/preprints/spacetime.pdf" target="_blank">Observation Selection Effects, Measures, and Infinite Spacetimes</a></h4>
              <p>An advanced Introduction to observation selection theory and its application to the cosmological fine-tuning problem. </p>
              <footer>[<strong>Universe or Multiverse?</strong>, ed. Bernard Carr (Cambridge University Press, 2007)] [<a href="http://anthropic-principle.com/preprints/spacetime.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.anthropic-principle.com/preprints/olum/sia.pdf" target="_blank">The Doomsday argument and the Self-Indication Assumption: Reply to Olum</a></h4>

              <p>Argues against Olum and the Self-Indication Assumption. </p>

							<footer>[w/ Milan Cirkovic] [<strong>Philosophical Quarterly</strong>, Vol. 53, No. 210 (2003): 83-91] [<a href="http://www.anthropic-principle.com/preprints/olum/sia.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.anthropic-principle.com/preprints/ali/alive.html" target="_blank">The Doomsday Argument is Alive and Kicking</a></h4>

              <p>Have Korb and Oliver refuted the doomsday argument? No. </p>

							<footer>[<strong>Mind</strong>, Vol. 108, No. 431 (1999): 539-550] [translation: <a href="http://www.proza.ru/texts/2007/05/20-267.html" target="_blank">Russian</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.anthropic-principle.com/preprints/cau/paradoxes.html" target="_blank">The Doomsday Argument, Adam &amp; Eve, UN++, and Quantum Joe</a></h4>

              <p>On the Doomsday argument and related paradoxes. </p>

							<footer>[<strong>Synthese</strong>, Vol. 127, No. 3 (2001): 359-387] [<a href="http://www.anthropic-principle.com/preprints/cau/paradoxes.html" target="_blank">html</a>] [<a href="http://www.anthropic-principle.com/preprints/cau/paradoxes.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.anthropic-principle.com/self-location.html" target="_blank">A Primer on the Doomsday argument</a></h4>

              <p>The Doomsday argument purports to prove, from basic probability theory and a few seemingly innocuous empirical premises, that the risk that our species will go extinct soon is much greater than previously thought. My view is that the Doomsday argument is inconclusive - although not for any trivial reason. In <a href="http://www.anthropic-principle.com/sites/anthropic-principle.com/files/pdfs/anthropicbias.pdf" target="_blank">my book</a>, I argued that a theory of observation selection effects is needed to explain where it goes wrong. </p>

							<footer>[<strong>Colloquia Manilana</strong> (PDCIS), Vol. 7 (1999)] [translation: <a href="http://www.proza.ru/texts/2007/04/13-34.html" target="_blank">Russian</a>]</footer>
            </article>
            <article>
              <h4><a href="http://www.anthropic-principle.com/preprints/beauty/synthesis.pdf" target="_blank">Sleeping Beauty and Self-Location: A Hybrid Model</a></h4>

              <p>The Sleeping Beauty problem is an important test stone for theories about self-locating belief. I argue against both the traditional views on this problem and propose a new synthetic approach. </p>

							<footer>[<strong>Synthese</strong>, Vol. 157, No. 1 (2007): 59-78] [<a href="http://www.anthropic-principle.com/preprints/beauty/synthesis.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.anthropic-principle.com/preprints/sowers/beyond.pdf" target="_blank">Beyond the Doomsday Argument: Reply to Sowers and Further Remarks</a></h4>

              <p>Argues against George Sower's refutation of the doomsday argument, and outlines what I think is the real flaw. </p>

							<footer>[<a href="http://www.anthropic-principle.com/preprints/sowers/beyond.pdf" target="_blank">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://plus.maths.org/issue17/features/traffic/index.html" target="_blank">Cars In the Other Lane Really Do Go Faster</a></h4>

              <p>When driving on the motorway, have you ever wondered about (and cursed!) the fact that cars in the other lane seem to be getting ahead faster than you? One might be tempted to account for this by invoking Murphy's Law ("If anything can go wrong, it will", discovered by Edward A. Murphy, Jr, in 1949). But there is an alternative explanation, based on observational selection effects&hellip; </p>

							<footer>[<strong>PLUS</strong>, No. 17 (2001)]</footer>
            </article>

            <article>
              <h4><a href="http://www.anthropic-principle.com/preprints/rel/relative.html" target="_blank">Observer-relative chances in anthropic reasoning?</a></h4>

              <p>A paradoxical thought experiment </p>

							<footer>[<strong>Erkenntnis</strong>, Vol. 52 (2000): 93-108]</footer>
            </article>

            <article>
              <h4><a href="https://arxiv.org/abs/gr-qc/9906042" target="_blank">Cosmological Constant and the Final Anthropic Hypothesis</a></h4>

              <p>Examines the implications of recent evidence for a cosmological constant for the prospects of indefinite information processing in the multiverse. Co-authored with Milan M. Cirkovic. </p>

							<footer>[<strong>Astrophysics and Space Science</strong>, Vol. 279, No. 4 (2000): 675-687] [<a href="https://arxiv.org/abs/gr-qc/9906042" target="_blank">pdf</a>]</footer>
            </article>

            <!-- #! Selected papers: Philosophy of Mind -->
            <h3>Philosophy of Mind</h3>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/experience.pdf" target="_self">Quantity of Experience: Brain-Duplication and Degrees of Consciousness</a></h4>

              <p>If two brains are in identical states, are there two numerically distinct phenomenal experiences or only one? Two, I argue. But what happens in intermediary cases? This paper looks in detail at this question and suggests that there can be a fractional (non-integer) number of qualitatively identical experiences. This has implications for what it is to implement a computation and for Chalmer's Fading Qualia thought experiment. </p>

							<footer>[<strong>Minds and Machines</strong>, Vol. 16, No. 2 (2006): 185-200] [<a href="http://www.nickbostrom.com/papers/experience.pdf" target="_self">pdf</a>]</footer>
            </article>

            <!-- #! Selected papers: Decision Theory -->
            <h3>Decision Theory</h3>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/newcomb.html" target="_self">The Meta-Newcomb Problem</a></h4>

              <p>A self-undermining variant of the Newcomb problem. </p>

							<footer>[<strong>Analysis</strong>, Vol. 61, No. 4 (2001): 309-310] [<a href="http://www.nickbostrom.com/papers/newcomb.html" target="_self">html</a>] [<a href="http://www.nickbostrom.com/papers/newcomb.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/pascal.pdf" target="_self">Pascal's Mugging</a></h4>

              <p>Finite version of Pascal's Wager. </p>

							<footer>[<strong>Analysis</strong>, Vol. 69, No. 3 (2009): 443-445] [translations: <a href="http://www.nickbostrom.com/translations/BraquagePascal.pdf" target="_self">French</a>, <a href="http://egzystencja.whus.pl/wp-content/uploads/2015/11/31-Bostrom-Napad-na-Pascala.pdf" target="_blank">Polish</a>] [<a href="http://www.nickbostrom.com/papers/pascal.pdf" target="_self">pdf</a>]</footer>
            </article>

          </div>
        </div>
      </section>
    </div>

    <!-- #! About section -->
    <div class="container">
      <section class="about" id="bio">
        <div class="row row-table">

          <!-- #! Bio column -->
          <div class="col-md-4">

            <div class="well well-bio bg-gray-lighter">
              <h3>Bio</h3>
				          <p>Nick Bostrom is a Swedish-born philosopher and polymath with a background in theoretical physics, computational neuroscience, logic, and artificial intelligence, as well as philosophy.  He is a Professor at Oxford University, where he leads the Future of Humanity Institute as its founding director.  (The FHI is a multidisciplinary university research centre; it is also home to the Centre for the Governance of Artificial Intelligence and to teams working on AI safety, biosecurity, macrostrategy, and various other technology or foundational questions.)  He is the author of some 200 publications, including <em>Anthropic Bias</em> (2002), <em>Global Catastrophic Risks</em> (2008), <em>Human Enhancement</em> (2009), and <em>Superintelligence: Paths, Dangers, Strategies</em> (2014), a New York Times bestseller which helped spark a global conversation about artificial intelligence.  Bostrom’s widely influential work, which traverses philosophy, science, ethics, and technology, has illuminated the links between our present actions and long-term global outcomes, thereby casting a new light on the human condition.
                  </p>
                  <p>He is recipient of a Eugene R. Gannon Award, and has been listed on Foreign Policy’s Top 100 Global Thinkers list twice.  He was included on Prospect’s World Thinkers list, the youngest person in the top 15.  His writings have been translated into 28 languages, and there have been more than 100 translations and reprints of his works.  He is a repeat TED speaker and has done more than 2,000 interviews with television, radio, and print media.  As a graduate student he dabbled in stand-up comedy on the London circuit, but he has since reconnected with the doom and gloom of his Swedish roots.</p>
                  <p>For more background, see profiles in e.g. <a href="http://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom" target="_blank">The New Yorker</a> or <a href="http://www.aeonmagazine.com/world-views/ross-andersen-human-extinction/" target="_blank">Aeon</a>.</p>

            </div>

            <div class="well well-crucial-considerations bg-gray-light">
              <h3>Work</h3>

              <p>My interests cut across many disciplines and may therefore at the surface appear somewhat scattered, but they all reflect a desire to figure out how to orient ourselves with respect to important values.  I refer to this as “macrostrategy”: the study of how long-term outcomes for humanity may be connected to present-day actions.  My research seeks to contribute to this by answering particular sub-questions or by developing conceptual tools that help us think about such questions more clearly.</p>

              <p>A key part of the challenge is often to notice that a problem even exists &ndash; to find it, formulate it, and then make enough initial progress in understanding it to let us break it into more tractable components and research tasks.  Much of my work (and that of the Future of Humanity Institute) operates in such a pre-paradigm environment.  We tend to work on problems that the rest of academia ignores either because the problems are not yet recognized as important or because it is unclear how one could conceivably go about doing research on them; and we try to advance understanding of them to the point where it becomes possible for a larger intellectual community to engage with them productively.  For example, a few years ago, AI alignment fell into this category: hardly anybody thought it was important, and it seemed like the kind of thing a science fiction author might write novels about but that there was no way to study scientifically.  By now, it has emerged as a bona fide research field, with people writing code and equations and making incremental progress.  Significant cognitive work was required to get to this point. </p>

              <div class="display-cont-with"><p><em>cont&rarr;</em></p></div>


              <!-- These line breaks control the height of the wells, because Tanya is bad at CSS-->
            </div>
          </div>

          <!-- #! Background column -->
          <div class="col-md-4">
            <div class="well well-background bg-gray-light">
              <!-- <div class="no-display-cont"><p style="text-align:center">&mdash;</p></div> -->
              <div class="display-cont-from"><p><em>cont&rarr;</em></p></div>

              <p>I have also originated or contributed to the development of ideas such as the simulation argument, existential risk, transhumanism, information hazards, superintelligence strategy, astronomical waste, crucial considerations, observation selection effects in cosmology and other contexts of self-locating belief, anthropic shadow, the unilateralist’s curse, the parliamentary model of decision-making under normative uncertainty, the notion of a singleton, the vulnerable world hypothesis, along with a number of analyses of future technological capabilities and concomitant ethical issues, risks, and opportunities.
              </p>

              <p>Technology is a theme in much of my work (and that of the FHI) because it is plausible that the long-term outcomes for our civilization depend sensitively on how we handle the introduction of certain transformative capabilities.  Machine intelligence, in particular, is a big focus.  We also work on biotechnology (both for its human enhancement applications and because of biosecurity concerns), nanotechnology, surveillance technology, and a bunch of other potential developments that could alter fundamental parameters of the human condition.
              </p>

              <p>There is a “why” beyond mere curiosity behind my interest in these questions, namely the hope that insight here may produce good effects.  In terms of directing our efforts as a civilization, it would seem useful to have some notion of which direction is “up” and which is “down”—what we should promote and what we should discourage.  Yet regarding macrostrategy, the situation is far from obvious.  We really have very little clue which of the actions available to present-day agents would increase or decrease the expected value of the long-term future, let alone which ones would do so the most effectively.  In fact, I believe it is likely that we are overlooking one or more crucial considerations: ideas or arguments that might plausibly reveal the need for not just some minor course adjustment in our endeavours but a major change of direction or priority.  If we have overlooked even just one such crucial consideration, then all our best efforts might be for naught—or they might even be making things worse.  Those seeking to make the world better should therefore take it as important to get to the bottom of these matters, or else to find some way of dealing wisely with our cluelessness if it is inescapable.
              </p>

              <p>The FHI works closely with the effective altruism community (e.g., we share office space with the Centre for Effective Altruism) as well as with AI leaders, philanthropic foundations, and other policymakers, scientists, and organizations to ensure that our research has impact.  These communication efforts are sometimes complicated by information hazard concerns.  Although many in the academic world take it as axiomatic that discovering and publishing truths is good, this assumption may be incorrect; certainly it may admit of exceptions.  For instance, if the world is vulnerable in some way, it may or may not be desirable to describe the precise way it is so.  I often feel like I’m frozen in an ice block of inhibition because of reflections of this sort.  How much easier things would be if one could have had a guarantee that all one’s outputs would be either positive or neutral, and one could go full blast!
              </p>



            </div>
          </div>

          <!-- #! Contact column -->
          <div class="col-md-4">
            <div class="well well-contact bg-gray-lighter">
              <h3>Contact</h3>

              <p>For administrative matters, scheduling, speaking engagements, etc., please contact my assistant:</p>

					<script type="text/javascript"> document.write('fhiea@' + 'philosophy.ox.ac' + /*h*/'.uk') </script>
					<noscript>
						<p>Email hidden by javascript, enable to view</p>
					</noscript>
					<p>+44 (0)1865 286800</p>
          <p>FHI is always looking for top talent. If you're interested, please see: <a href="https://www.fhi.ox.ac.uk/vacancies/" target="_blank">https://www.fhi.ox.ac.uk/vacancies/</a></p>
			   <br />

              <p>If you need to contact me directly (I regret I am not always able to respond to emails):</p>
				<noscript><p>Email hidden by javascript, enable to view.</p></noscript>
					<script type="text/javascript"> document.write('nick.bostrom@' + 'philosophy.ox' + /*h*/'.ac.uk') </script>

				<br />

              <h3>Virtual Estate</h3>

              <p><strong><a href="https://www.simulation-argument.com" target="_blank">www.simulation-argument.com</a></strong><em>&mdash;</em>Devoted to the question, "Are you living in a computer simulation?"</p>
              <p><strong><a href="http://www.fhi.ox.ac.uk" target="_blank">www.fhi.ox.ac.uk</a></strong><em>&mdash;</em>Future of Humanity Institute</p>
              <p><strong><a href="http://www.anthropic-principle.com" target="_blank">www.anthropic-principle.com</a></strong><em>&mdash;</em>Papers on observational selection effects</p>
              <p><strong><a href="http://www.existential-risk.org" target="_blank">www.existential-risk.org</a></strong><em>&mdash;</em>Human extinction scenarios and related concerns</p>

              <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
              <p style="margin-left: 100px; margin-top: 70px; text-align: left;">
               ON THE BANK</br>
             </br>
                On the bank at the end<br>
                Of what was there before us<br>
                Gazing over to the other side<br>
                On what we can become<br>
                Veiled in the mist of na&iuml;ve speculation<br>
                We are busy here preparing<br>
                Rafts to carry us across<br>
                Before the light goes out leaving us<br>
                In the eternal night of could-have-been</p>

                <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
            </div>
          </div>

        </div>
      </section>
    </div>

    <div class="container">
      <section class="everything-else">
        <!-- #! Everything else section -->

        <!--
        <div class="row section-header">
          <div class="col-sm-12">
            <h2>Everything else</h2>
          </div>
        </div> -->

        <div class="row">

          <!-- #! Videos and lectures column -->
          <div class="col-md-4">
            <h3>Some Videos &amp; Lectures</h3>

						<article>
              <h4><a href="https://www.ted.com/talks/nick_bostrom_how_civilization_could_destroy_itself_and_4_ways_we_could_prevent_it" target="_blank" onclick="ga('send','event','vwh','click','ted2019_title')">TED2019</a>&nbsp;<img src="star.jpg">&nbsp;<img src="new-icon-6.jpg" align="bottom"></h4>
							<div style="max-width:854px;padding:8px 0 8px">
								<div style="position:relative;height:0;padding-bottom:56.25%">
									<iframe src="https://embed.ted.com/talks/nick_bostrom_how_civilization_could_destroy_itself_and_4_ways_we_could_prevent_it" width="854" height="480" style="position:absolute;left:0;top:0;width:100%;height:100%" frameborder="0" scrolling="no" allowfullscreen onclick="ga('send','event','vwh','click','ted2019_embedded')">
									</iframe>
								</div>
							</div>

							<p>Professor Nick Bostrom chats about the vulnerable world hypothesis with Chris Anderson.</p>
              <footer>(<strong>TED</strong>, April 2019), 21 mins</footer>
            </article>

            <article>
              <h4><a href="https://www.youtube.com/watch?v=8xwESiI6uKA" target="_blank">The Future of Machine Intelligence</a>&nbsp;<img src="star.jpg"></h4>

              <div class="embed-responsive embed-responsive-16by9">
              <iframe width="320" height="180" src="https://www.youtube.com/embed/8xwESiI6uKA?rel=0" frameborder="0" allowfullscreen></iframe>
              </div>

              <p>Talk at the 2017 USI Conference</p>

              <footer>(Paris, July 2017), 41 mins</footer>

            </article>

            <article>
              <h4><a href="https://www.youtube.com/watch?v=4CEkd4zG8Yo" target="_blank">Interview by Adam Ford</a>&nbsp;<img src="star.jpg"></h4>

              <div class="embed-responsive embed-responsive-16by9">
                <iframe width="320" height="180" src="https://www.youtube.com/embed/4CEkd4zG8Yo" frameborder="0" allowfullscreen></iframe>
              </div>

              <p>Wide-ranging interview</p>

              <footer>(23 February 2013), 51 mins</footer>
            </article>
			<article>
				<h4><a href="https://www.youtube.com/watch?v=MnT1xgZgkpk" target="_blank">TED talk on AI risk</a></h4>

				<div class="embed-responsive embed-responsive-16by9">
                  <iframe width="320" height="180" src="https://www.youtube.com/embed/MnT1xgZgkpk" frameborder="0" allowfullscreen></iframe>
                </div>
                <p>My second TED talk</p>

              <footer>(Vancouver 2015), 17 mins</footer>
			</article>

      <article>
				<h4><a href="https://soundcloud.com/gooddoneright/nick-bostrom-crucial-considerations-and-wise-philanthropy" target="_blank">Crucial Considerations and Wise Philanthropy</a></h4>
        <p>What is a crucial consideration?</p>
        <footer>
          <p>[Oxford, 9th July 2014] [<a href="https://nickbostrom.com/lectures/crucial_final.pdf">slides</a>] [<a href="https://soundcloud.com/gooddoneright/nick-bostrom-crucial-considerations-and-wise-philanthropy" target="_blank">audio</a>]</p>
        </footer>
			</article>

            <h3>Some additional (old, cobwebbed) papers</h3>

             <article>
            <p>On <a href="http://www.nickbostrom.com/oldpapers.html" target="_self">this</a> page.</p>
            <br />
            <br />
            <br />
            <br />
            </article>

          </div>

          <!-- #! Interviews column -->
          <div class="col-md-4">
            <h3>Interviews</h3>

            <article>
              <header>
                <a href="http://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom" target="_blank" class="book__cover"><img src="https://www.fhi.ox.ac.uk/wp-content/uploads/The-new-yorker-logo-2-150x35.png"></a>
                <h4><a href="http://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom" target="_blank">The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?</a>&nbsp;<img src="star.jpg"></h4>
              </header>
              <p>
                A long-form feature profile of me, by Raffi Khatchadourian.
              </p>
              <footer>[<strong>The New Yorker</strong>, November 23 issue (2015)]</footer>
            </article>

            <article>
              <h4><a href="http://www.aeonmagazine.com/world-views/ross-andersen-human-extinction/" target="_blank">Omens</a>&nbsp;<img src="star.jpg"></h4>

              <p>Long article by Ross Andersen about the work of the Future of Humanity Institute</p>

              <footer>(February, 2013)</footer>
            </article>

            <article>
              <h4><a href="http://80000hours.org/blog/140-how-to-make-a-difference-in-research-an-interview-with-nick-bostrom" target="_blank">How to make a difference in research: interview for 80,000 Hours</a></h4>

              <p>Interview for the meta-charity 80,000 Hours on how to make a maximally positive impact on the world for people contemplating an academic career trajectory</p>

              <footer>(January, 2013)</footer>
            </article>

            <article>
              <h4><a href="http://traffic.libsyn.com/philosophybites/Nick_Bostrom_on_the_Simulation_Argument.mp3" target="_blank">On the simulation argument</a>&nbsp;<img src="star.jpg"></h4>

              <p>15-minute audio interview explaining the simulation argument.</p>

							<footer>(Philosophy Bites, 2011) [<a href="http://traffic.libsyn.com/philosophybites/Nick_Bostrom_on_the_Simulation_Argument.mp3" target="_blank">mp3</a>]</footer>
            </article>

            <article>
              <h4><a href="http://media.philosophy.ox.ac.uk/bioethicsbites/Bostrom.mp3" target="_blank">On cognitive enhancement and status quo bias</a></h4>

              <p>15-minute interview about status quo bias in bioethics, and the "reversal test" by which such bias might be cured.</p>

							<footer>(Bioethics, 2011) [<a href="http://media.philosophy.ox.ac.uk/bioethicsbites/Bostrom.mp3" target="_blank">mp3</a>]</footer>
            </article>

            <!-- <article>
              <h4><a href="http://cyberlaw.stanford.edu/podcasts/20101201_Levine_128_Bostrom.mp3" target="_blank">50-min interview on Hearsay Culture</a></h4>

              <p>Covering Future of Humanity Institute, crucial considerations, existential risks, information hazards, and academic specialization. Interviewed by Prof. Dave Levine, KZSU-FM.</p>

							<footer>(Stanford University, 2010) [<a href="http://cyberlaw.stanford.edu/podcasts/20101201_Levine_128_Bostrom.mp3" target="_blank">mp3</a>]</footer>
            </article> -->

            <article>
              <h4><a href="http://www.theeuropean-magazine.com/nick-bostrom--2/6028-genetic-enhancement-and-the-future-of-humanity" target="_blank">Interview for The European</a></h4>

              <p>Interviewed by Martin Eiermann about existential risks, genetic enhancements, and ethical discourses about technological progress.</p>

              <footer>(13 June 2011)</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/views/science.pdf" target="_self">Three Ways to Advance Science</a></h4>

							<footer>[<strong>Nature</strong> Podcast, 31 January 2008] [<a href="http://www.nickbostrom.com/views/science.pdf" target="_self">pdf</a>] [<a href="http://media.nature.com/download/nature/nature/podcast/v451/n7178/nature-2008-01-31.mp3" target="_blank">mp3</a>] (my segment starts at 19:30)</p>
            </article>

            <h3>Policy</h3>
            <article>
              <h4><a href="http://www.nickbostrom.com/views/identity.pdf" target="_self">The Future of Identity</a></h4>

              <p>On the future of "human identity" in relation to information and communication technologies, automation and robotics, and biotechnology and medicine.</p>

							<footer>[w/ Anders Sandberg] [<strong>Report, Commissioned by the UK's Government Office for Science</strong>, 2011] [<a href="http://www.nickbostrom.com/views/identity.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/smart-policy.pdf" target="_self">Smart Policy: Cognitive Enhancement and the Public Interest</a></h4>

              <p>Summarizing some of the key issues and offering policy recommendations for a "smart policy" on biomedical methods of enhancing cognitive performance.</p>

							<footer>[w/ Rebecca Roache] [<strong>Enhancing Human Capacities</strong>, eds. J. Savulescu, R. ter Muelen, &amp; G. Kahane (Oxford: Wiley-Blackwell, 2009)] [<a href="http://www.nickbostrom.com/papers/smart-policy.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/views/whyfriendlyai.pdf" target="_self">Why we need Friendly AI</a></h4>

              <p>Humans will not always be the most intelligent agents on Earth, the ones steering the future. What will happen to us when we no longer play that role, and how can we prepare for this transition? </p>

							<footer>[w/ Luke Muehlhauser] [<strong>Think</strong>, Vol. 13, No. 36 (2013): 41-47] [<a href="http://www.nickbostrom.com/views/whyfriendlyai.pdf">pdf</a>] [translation: <a href="https://nickbostrom.com/translations/Russian_friendlyAI.pdf" target="_self">Russian</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/views/science.pdf" target="_self">Three Ways to Advance Science</a></h4>

              <p>Those who seek the advancement of science should focus more on scientific research that facilitates further research across a wide range of domains<em>&mdash;</em>particularly cognitive enhancement.</p>

							<footer>[<strong>Nature</strong> Podcast, 31 January 2008] [<a href="http://www.nickbostrom.com/views/science.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/letters/drugs.pdf" target="_self">Drugs can be used to treat more than disease</a></h4>

              <p>Short letter to the editor on obstacles to the development of better cognitive enhancement drugs. </p>

							<footer>[<strong>Nature</strong>, Vol. 452, No. 7178 (2008): 520] [<a href="http://www.nickbostrom.com/letters/drugs.pdf" target="_self">pdf</a>]</footer>
            </article>

          </div>

          <!-- #! Misc column -->
          <div class="col-md-4 col-last">
            <h3>Miscellaneous</h3>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/interests-of-digital-minds.pdf" target="_self">The Interests of Digital Minds</a></h4>
              <p>A blog post draft. </p>
              <footer>[draft 1.0 (2018)] [<a href="http://www.nickbostrom.com/papers/interests-of-digital-minds.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/fable/retriever.html" target="_self">Golden</a></h4>
              <p>Fictional interview of an uploaded dog by Larry King. </p>
							<footer>[2004] [<a href="http://www.nickbostrom.com/fable/retriever.html" target="_self">html</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/poetry/synkrotron.html" target="_self">Synkrotron</a></h4>
              <p>A poetry cycle&hellip; in Swedish, unfortunately. I stopped writing poetry after this, although I've had a few <a href="http://www.nickbostrom.com/poetry/poetry.html" target="_self">relapses</a> in the English language.</p>
              <footer>[1995] [<a href="http://www.nickbostrom.com/poetry/synkrotron.html" target="_self">html</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/2050/world.html" target="_self">The World in 2050</a></h4>

              <p>Imaginary dialogue, set in the year 2050, in which three pundits debate the big issues of their time </p>

							<footer>[2000] [<a href="http://www.nickbostrom.com/2050/world.html" target="_self">html</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/dangerous.html" target="_self">Transhumanism: The World's Most Dangerous Idea?</a></h4>

              <p>According to Francis Fukuyama, yes. This is my response. </p>

							<footer>[Short version in <strong>Foreign Policy</strong>; full version in <strong>Betterhumans</strong>, issue 10/19/2004] [<a href="http://www.nickbostrom.com/papers/dangerous.html" target="_self">html</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/reviews/neuroethics.pdf" target="_self">Moralist, meet Scientist</a></h4>

              <p>Review of Kwame Anthony Appiah's book "Experiments in Ethics". </p>

							<footer>[<strong>Nature</strong>, Vol. 453 (2008): 593-594] [<a href="http://www.nickbostrom.com/reviews/neuroethics.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/superintelligence.html" target="_self">How Long Before Superintelligence?</a></h4>

              <p>This paper, now a few years old, examines how likely it might be that we will develop superhuman artificial intelligence within the first third of this century. </p>

							<footer>[Updated version (2008) of the original in <strong>Int. J. of Future Studies</strong>, Vol. 2 (1998)] [<a href="http://www.nickbostrom.com/superintelligence.html" target="_self">html</a>] [translation: <a href="http://alt-future.narod.ru/Future/superint.htm" target="_blank">Russian</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/2050/outsmart.html" target="_self">When Machines Outsmart Humans</a></h4>

              <p>This slightly more recent (but still obsolete) article briefly reviews the argument set out in the previous one, and notes four immediate consequences of human-level machine intelligence. </p>

							<footer>[<strong>Futures</strong>, Vol. 35, No. 7 (2003): 759-764, where it appears as the target paper of a symposium, together with five commentaries by other people, to which I had the opportunity to <a href="http://www.nickbostrom.com/2050/reply.html" target="_self">reply</a> in the subsequent issue.] [<a href="http://www.nickbostrom.com/2050/outsmart.html" target="_self">html</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/views/everything.pdf" target="_self">Everything</a></h4>

              <p>Response to 2008 Edge Question: "What have you changed your mind about?" </p>

							<footer>[2008] [<a href="http://www.nickbostrom.com/views/everything.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/views/superintelligence.pdf" target="_self">Superintelligence</a></h4>

              <p>Response to 2009 Edge Question: "What will change everything?" </p>

							<footer>[2009] [<a href="http://www.nickbostrom.com/views/superintelligence.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/views/internet.pdf" target="_self">Most Still to Come</a></h4>

              <p>Response to 2010 Edge Question: "How is the Internet changing the way you think?" </p>

							<footer>[2010] [<a href="http://www.nickbostrom.com/views/internet.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/papers/globalagenda.pdf" target="_self">Dinosaurs, Dodos, Humans?</a></h4>

              <p>Short article on existential risks. </p>

							<footer>[<strong>Global Agenda</strong>, Feb (2006): 230-231; the annual publication of the World Economic Forum] [<a href="http://www.nickbostrom.com/papers/globalagenda.pdf" target="_self">pdf</a>]</footer>
            </article>

            <article>
              <h4><a href="http://www.nickbostrom.com/views/generators.pdf" target="_self">The Game of Life - And Looking for Generators</a></h4>

              <p>Response to 2011 Edge Question: "What scientific concept would improve everybody's cognitive toolkit?" </p>

							<footer>[2011] [<a href="http://www.nickbostrom.com/views/generators.pdf" target="_self">pdf</a>]</footer>
            </article>

            <!-- <article>
              <h4><a href="https://www.nickbostrom.com/interviews/Sommarprat-P1.pdf" target="_self">Some autobiographical fragments, in Swedish</a></h4>

              <p>Transcript of radio program.</p>

              <footer>[<a href="https://www.nickbostrom.com/interviews/Sommarprat-P1.pdf" target="_self">pdf</a>]</footer>
            </article> -->

            <p><br /><br /><br /><a href="https://www.nickbostrom.com/poetry/poetry.html" target="_self"><img src="images/poetrylogo13.jpg"></a></p>

          </div>
        </div>
      </section>
    </div>



    <script>
    if(window.location.href.indexOf('alt_portrait') !== -1) {
      var portrait = document.querySelectorAll('.site-header img')[0];
      console.log(portrait);
      portrait.src = 'nick-bostrom-ft-portrait.jpg';

      var portraitWrapper = document.querySelectorAll('.site-header .col-md-4')[0];

      var siteTitleWrapper = document.querySelectorAll('.site-header .col-md-8')[0];

      portraitWrapper.className = 'col-xxs-12 col-xs-6 col-md-2';

      siteTitleWrapper.className = 'col-xxs-12 col-xs-6 col-md-10 col-last col-site-header__title';
      siteTitleWrapper.style = 'margin-left: 16.666666667%';
    }
    </script>

  </body>
</html>
